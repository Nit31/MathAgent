{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import os\n",
    "from typing import Annotated\n",
    "\n",
    "import sympy as sp\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini-2024-07-18', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sympy-based tool\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate an arithmetic expression using sympy.\n",
    "\n",
    "    Args:\n",
    "        expression: arithmetic expression as a string (e.g., '3 + 4')\n",
    "\n",
    "    Returns:\n",
    "        The evaluated result as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expr = sp.sympify(expression)\n",
    "        evaluated = expr.evalf() if expr.is_number else expr\n",
    "        return str(evaluated)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Define a LLM tool\n",
    "@tool\n",
    "def llm_tool(expression: str) -> str:\n",
    "    \"\"\"Use LLM like yourself to process the input string.\n",
    "    This is useful for tasks that require reasoning or understanding of the context.\n",
    "    For example, you can use it to provide explanations.\n",
    "\n",
    "    Args:\n",
    "        expression: input string to be processed by LLM\n",
    "\n",
    "    Returns:\n",
    "        The processed output as a string.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([SystemMessage(content=expression)])\n",
    "    return response.content\n",
    "\n",
    "# Augment the LLM with the sympy tool\n",
    "tools = [calculate, llm_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "James is a first-year student at a University in Chicago. He has a budget of $1000 per semester. He spends 30% of his money on food, 15% on accommodation, 25% on entertainment, and the rest on coursework materials. How much money does he spend on coursework materials?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  calculate (call_2cYqxtJIyq2yaswVnbEmdvYJ)\n",
      " Call ID: call_2cYqxtJIyq2yaswVnbEmdvYJ\n",
      "  Args:\n",
      "    expression: 1000 * (1 - 0.30 - 0.15 - 0.25)\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: calculate\n",
      "\n",
      "\"300.000000000000\"\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "James spends $300 on coursework materials.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Answer: 300\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def chatbot(state: AgentState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "def route_tools(\n",
    "    state: AgentState,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "def format_answer(state: AgentState):\n",
    "    \"\"\"Final answer formatter using llm_tool\"\"\"\n",
    "    last_ai_message = next(\n",
    "        msg for msg in reversed(state[\"messages\"])\n",
    "        if isinstance(msg, AIMessage)\n",
    "    )\n",
    "\n",
    "    format_prompt = f\"\"\"\n",
    "    REFINE THIS ANSWER TO EXACT FORMAT:\n",
    "    Original: {last_ai_message.content}\n",
    "\n",
    "    Rules:\n",
    "    1. Extract ONLY the numeric answer\n",
    "    2. Format EXACTLY as: \"Answer: <number>\"\n",
    "    3. Never add explanations or other text\n",
    "    \"\"\"\n",
    "\n",
    "    formatted = llm_tool(format_prompt)\n",
    "\n",
    "    return {\"messages\": [HumanMessage(content=formatted)]}\n",
    "\n",
    "tool_node = BasicToolNode(tools=tools)\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"format_answer\", format_answer)\n",
    "\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: \"format_answer\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"chatbot\")\n",
    "workflow.add_edge(\"format_answer\", END)\n",
    "\n",
    "agent = workflow.compile()\n",
    "\n",
    "task = \"James is a first-year student at a University in Chicago. He has a budget of $1000 per semester. He spends 30% \\\n",
    "of his money on food, 15% on accommodation, 25% on entertainment, and the rest on coursework materials. How much money \\\n",
    "does he spend on coursework materials?\"\n",
    "\n",
    "\n",
    "# for m in agent.stream({\"messages\": [HumanMessage(content=task)]}):\n",
    "#     print(m)\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=task)]})\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "# test_dataset = dataset[\"test\"]\n",
    "# val_sample = test_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# def check_answer(model_answer, true_answer):\n",
    "#     try:\n",
    "#         true_answer = true_answer.split(',')\n",
    "\n",
    "#         true_answer = [int(x) for x in true_answer]\n",
    "#     except Exception:\n",
    "#         true_answer = [int(true_answer)]\n",
    "\n",
    "#     return int(model_answer) in true_answer\n",
    "\n",
    "# k = 0\n",
    "# model_answers = []\n",
    "\n",
    "# for problem in tqdm(val_sample):\n",
    "#     # Get the math problem and the correct answer\n",
    "#     math_problem = problem['question']\n",
    "#     correct_answer = problem['answer'].split(\"### \")[1]\n",
    "\n",
    "#     # Generate the model's response using your agent\n",
    "#     state = agent.invoke({\"messages\": [HumanMessage(content=math_problem)]})\n",
    "#     model_response = state[\"messages\"][-1]  # Get final response\n",
    "#     model_answers.append(model_response.content)\n",
    "\n",
    "#     # Use regex to parse the numerical answer exactly as before\n",
    "#     try:\n",
    "#         model_ans = re.search(r'Answer:\\s*[^0-9]*([\\d]+(?:\\.\\d+)?)', model_response.content).group(1).strip()\n",
    "#         k += 1 if check_answer(model_ans, correct_answer) else 0\n",
    "#     except Exception:\n",
    "#         continue\n",
    "\n",
    "# print(f\"Precision: {k/len(val_sample)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

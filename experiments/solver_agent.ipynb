{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import sympy as sp\n",
    "from datasets import load_dataset\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini-2024-07-18', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single sympy-based tool\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate an arithmetic expression using sympy.\n",
    "\n",
    "    Args:\n",
    "        expression: arithmetic expression as a string (e.g., '3 + 4')\n",
    "\n",
    "    Returns:\n",
    "        The evaluated result as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expr = sp.sympify(expression)\n",
    "        evaluated = expr.evalf() if expr.is_number else expr\n",
    "        return str(evaluated)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Define a LLM tool\n",
    "@tool\n",
    "def llm_tool(expression: str) -> str:\n",
    "    \"\"\"Use LLM like yourself to process the input string.\n",
    "    This is useful for tasks that require reasoning or understanding of the context.\n",
    "    For example, you can use it to provide explanations.\n",
    "\n",
    "    Args:\n",
    "        expression: input string to be processed by LLM\n",
    "\n",
    "    Returns:\n",
    "        The processed output as a string.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([SystemMessage(content=expression)])\n",
    "    return response.content\n",
    "\n",
    "# Augment the LLM with the sympy tool\n",
    "tools = [calculate, llm_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_system = \"\"\"\n",
    "### INSTRUCTIONS ###\n",
    "1. You are a math problem solver. You are given a math problem and you need to solve it using two tools.\n",
    "2.Respond with the answer in a format: \"Answer: <value>\". Value should be a number without any units. \\\n",
    "If you are not sure about the answer, respond with \"I don't know\".\n",
    "\n",
    "TOOLS:\n",
    "(1) Calculate[input]: A tool that is used for solving math expressions using sympy.\n",
    "(2) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and \\\n",
    "common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
    "\n",
    "### EXAMPLE ###\n",
    "## TASK\n",
    "Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there \\\n",
    "are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How \\\n",
    "many flowers does Mark have in his garden?\n",
    "## ANSWER\n",
    "There are 80/100 * 10 = <<80/100*10=8>>8 more purple flowers than yellow flowers.\n",
    "So in Mark's garden, there are 10 + 8 = <<10+8=18>>18 purple flowers.\n",
    "Purple and yellow flowers sum up to 10 + 18 = <<10+18=28>>28 flowers.\n",
    "That means in Mark's garden there are 25/100 * 28 = <<25/100*28=7>>7 green flowers.\n",
    "So in total Mark has 28 + 7 = <<28+7=35>>35 plants in his garden.\n",
    "Answer: 35\n",
    "\"\"\"\n",
    "\n",
    "prompt_human = \"\"\"\n",
    "### YOUR TASK ###\n",
    "Solve the following math problem\n",
    "TASK: {task}\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_system),\n",
    "    (\"human\", prompt_human)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 45\u001b[0m\n\u001b[0;32m     39\u001b[0m agent \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m     41\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJames is a first-year student at a University in Chicago. He has a budget of $1000 per semester. He spends 30\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124mof his money on food, 15\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mn accommodation, 25\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mn entertainment, and the rest on coursework materials. How much money \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124mdoes he spend on coursework materials?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(m\u001b[38;5;241m.\u001b[39mpretty_print())\n",
      "File \u001b[1;32mc:\\Users\\Elena\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2683\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2682\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2683\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2684\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2687\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2688\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2691\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2693\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elena\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2351\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2343\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[0;32m   2344\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   2345\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2349\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[0;32m   2350\u001b[0m     )\n\u001b[1;32m-> 2351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[0;32m   2353\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[1;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to use tools or answer directly\"\"\"\n",
    "    task = next(msg.content for msg in state[\"messages\"] if isinstance(msg, HumanMessage))\n",
    "    messages = prompt_template.format_messages(task=task)\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Execute tool calls\"\"\"\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": state[\"messages\"] + result}\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"end\"]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    return \"tools\" if last_message.tool_calls else \"end\"\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"llm_call\", llm_call)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"llm_call\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"llm_call\")\n",
    "\n",
    "agent = workflow.compile()\n",
    "\n",
    "task = \"James is a first-year student at a University in Chicago. He has a budget of $1000 per semester. He spends 30% \\\n",
    "of his money on food, 15% on accommodation, 25% on entertainment, and the rest on coursework materials. How much money \\\n",
    "does he spend on coursework materials?\"\n",
    "\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=task)]})\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    print(m.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "# test_dataset = dataset[\"test\"]\n",
    "# val_sample = test_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# def check_answer(model_answer, true_answer):\n",
    "#     try:\n",
    "#         true_answer = true_answer.split(',')\n",
    "\n",
    "#         true_answer = [int(x) for x in true_answer]\n",
    "#     except Exception:\n",
    "#         true_answer = [int(true_answer)]\n",
    "\n",
    "#     return int(model_answer) in true_answer\n",
    "\n",
    "# k = 0\n",
    "# model_answers = []\n",
    "\n",
    "# for problem in tqdm(val_sample):\n",
    "#     # Get the math problem and the correct answer\n",
    "#     math_problem = problem['question']\n",
    "#     correct_answer = problem['answer'].split(\"### \")[1]\n",
    "\n",
    "#     # Generate the model's response using your agent\n",
    "#     state = agent.invoke({\"messages\": [HumanMessage(content=math_problem)]})\n",
    "#     model_response = state[\"messages\"][-1]  # Get final response\n",
    "#     model_answers.append(model_response.content)\n",
    "\n",
    "#     # Use regex to parse the numerical answer exactly as before\n",
    "#     try:\n",
    "#         model_ans = re.search(r'Answer:\\s*[^0-9]*([\\d]+(?:\\.\\d+)?)', model_response.content).group(1).strip()\n",
    "#         k += 1 if check_answer(model_ans, correct_answer) else 0\n",
    "#     except Exception:\n",
    "#         continue\n",
    "\n",
    "# print(f\"Precision: {k/len(val_sample)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

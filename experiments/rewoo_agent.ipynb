{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, TypedDict\n",
    "\n",
    "import sympy as sp\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini-2024-07-18', temperature=0, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: List[MessagesState]\n",
    "    task: str\n",
    "    plan_string: str\n",
    "    steps: List\n",
    "    results: Dict\n",
    "    result: str\n",
    "\n",
    "\n",
    "# Define a sympy-based tool\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate an arithmetic expression using sympy.\n",
    "\n",
    "    Args:\n",
    "        expression: arithmetic expression as a string (e.g., '3 + 4')\n",
    "\n",
    "    Returns:\n",
    "        The evaluated result as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expr = sp.sympify(expression)\n",
    "        evaluated = expr.evalf() if expr.is_number else expr\n",
    "        return str(evaluated)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Define a LLM tool\n",
    "@tool\n",
    "def llm_tool(expression: str) -> str:\n",
    "    \"\"\"Use LLM like yourself to process the input string.\n",
    "    This is useful for tasks that require reasoning or understanding of the context.\n",
    "    For example, you can use it to provide explanations.\n",
    "\n",
    "    Args:\n",
    "        expression: input string to be processed by LLM\n",
    "\n",
    "    Returns:\n",
    "        The processed output as a string.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([SystemMessage(content=expression)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Augment the LLM with the sympy tool\n",
    "tools = [calculate, llm_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "planer_prompt_system = \"\"\"\n",
    "### INSTRUCTIONS ###\n",
    "You are a math problem solving planner. For the following task, make plans that can solve \\\n",
    "the problem step by step. For each plan, indicate which external tool together with tool input to retrieve \\\n",
    "evidence. You can store the evidence into a variable #E that can be called by later tools \\\n",
    "(Plan, #E1, Plan, #E2, Plan, ...)\n",
    "\n",
    "Tools can be one of the following:\n",
    "(1) Calculate[input]: A tool that is used for solving math expressions using sympy.\n",
    "(2) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and \\\n",
    "common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
    "\n",
    "### EXAMPLE ###\n",
    "Task: A person has $500. They spend 40% on groceries, 20% on utilities, and 10% on transportation. \\\n",
    "How much money do they have left?\n",
    "\n",
    "Plan: Calculate the total amount spent on groceries, utilities, and transportation.\n",
    "#E1 = Calculate[0.40 * 500 + 0.20 * 500 + 0.10 * 500]\n",
    "\n",
    "Plan: Subtract the total amount spent from the initial amount to find the remaining money.\n",
    "#E2 = Calculate[500 - #E1]\n",
    "\"\"\"\n",
    "\n",
    "planer_prompt_human = \"\"\"\n",
    "### YOUR TASK ###\n",
    "Describe your plans with rich details. Each Plan should be followed by only one #E.\n",
    "\n",
    "Task: {task}\n",
    "Plan:\n",
    "\"\"\"\n",
    "\n",
    "planer_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", planer_prompt_system),\n",
    "        (\"user\", planer_prompt_human)\n",
    "    ]\n",
    ")\n",
    "\n",
    "solver_prompt = \"\"\"\n",
    "### INSTRUCTIONS ###\n",
    "Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \\\n",
    "retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \\\n",
    "contain irrelevant information.\n",
    "\n",
    "Respond with the answer in a format: \"Answer: <value>\". Value should be a number without \\\n",
    "any units. If you are not sure about the answer, respond with \"I don't know\".\n",
    "\n",
    "### TASK ###\n",
    "{task}\n",
    "\n",
    "### PLAN ###\n",
    "{plan}\n",
    "\n",
    "### ANSWER ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner node\n",
    "def planner(state: AgentState):\n",
    "    \"\"\"Generate a step-by-step plan to solve the problem\"\"\"\n",
    "    regex_pattern = r\"Plan:\\s*(.+)\\s*(#E\\d+)\\s*=\\s*(\\w+)\\s*\\[([^\\]]+)\\]\"\n",
    "\n",
    "    task = state[\"task\"]\n",
    "\n",
    "    prompt = planer_prompt_template.format_messages(task=task)\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extract the plan string from the response\n",
    "    matches = re.findall(regex_pattern, response.content)\n",
    "    return {\"steps\": matches, \"plan_string\": response.content, \"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "def _get_current_task(state: AgentState):\n",
    "    if \"results\" not in state or state[\"results\"] is None:\n",
    "        return 1\n",
    "    if len(state[\"results\"]) == len(state[\"steps\"]):\n",
    "        return None\n",
    "    else:\n",
    "        return len(state[\"results\"]) + 1\n",
    "\n",
    "# Executor node\n",
    "def executor(state: AgentState):\n",
    "    \"\"\"Worker node that executes the tools of a given plan.\"\"\"\n",
    "    _step = _get_current_task(state)\n",
    "    try:\n",
    "        _, step_name, tool_name, tool_input = state[\"steps\"][_step - 1]\n",
    "    except IndexError:\n",
    "        # There was an error in the plan\n",
    "        return {\"messages\": state[\"messages\"] + [SystemMessage(content=\"Error in the plan.\")]}\n",
    "\n",
    "    _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "    for k, v in _results.items():\n",
    "        tool_input = tool_input.replace(k, v)\n",
    "    if tool_name == \"Calculate\":\n",
    "        result = calculate.invoke(tool_input)\n",
    "    elif tool_name == \"LLM\":\n",
    "        result = llm_tool.invoke(tool_input)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    _results[step_name] = str(result)\n",
    "\n",
    "    tool_message = ToolMessage(content=f\"{tool_input}\\nResult: {result}\", artifact=result, tool_call_id=step_name)\n",
    "\n",
    "    return {\"results\": _results, \"messages\": state[\"messages\"] + [tool_message]}\n",
    "\n",
    "def solve(state: AgentState):\n",
    "    plan = \"\"\n",
    "    for _plan, step_name, tool_name, tool_input in state[\"steps\"]:\n",
    "        _results = (state[\"results\"] or {}) if \"results\" in state else {}\n",
    "        for k, v in _results.items():\n",
    "            tool_input = tool_input.replace(k, v)\n",
    "            step_name = step_name.replace(k, v)\n",
    "        plan += f\"Plan: {_plan}\\n{step_name} = {tool_name}[{tool_input}]\"\n",
    "    prompt = solver_prompt.format(plan=plan, task=state[\"task\"])\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"result\": result.content, \"messages\": state[\"messages\"] + [result]}\n",
    "\n",
    "def _route(state: AgentState):\n",
    "    _step = _get_current_task(state)\n",
    "    if _step is None:\n",
    "        # We have executed all tasks\n",
    "        return \"solve\"\n",
    "    else:\n",
    "        # We are still executing tasks, loop back to the \"tool\" node\n",
    "        return \"tool\"\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"plan\", planner)\n",
    "graph.add_node(\"tool\", executor)\n",
    "graph.add_node(\"solve\", solve)\n",
    "\n",
    "graph.add_edge(\"plan\", \"tool\")\n",
    "graph.add_edge(\"solve\", END)\n",
    "graph.add_conditional_edges(\"tool\", _route)\n",
    "graph.add_edge(START, \"plan\")\n",
    "\n",
    "agent = graph.compile()\n",
    "\n",
    "\n",
    "task = \"Indras has 6 letters in her name. Her sister's name has 4 more letters than half of the letters in Indras' name. How many letters are in Indras and her sister's names?\"\n",
    "\n",
    "messages = [HumanMessage(content=task)]\n",
    "\n",
    "# result = agent.invoke({\"task\": task, \"messages\": messages})\n",
    "# # Print full conversation with steps\n",
    "# for m in result[\"messages\"]:\n",
    "#     m.pretty_print()\n",
    "\n",
    "# # Print full conversation with steps\n",
    "# for m in agent.stream({\"task\": task, \"messages\": messages}):\n",
    "#     print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/.pyenv/versions/3.11.10/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "# Access the training and test splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "val_sample = test_dataset.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def check_answer(model_answer, true_answer):\n",
    "    true_answer = true_answer.replace(\",\", \"\")\n",
    "\n",
    "    return str(model_answer) in true_answer\n",
    "\n",
    "\n",
    "def process_problem(problem):\n",
    "    # Get the math problem and the correct answer\n",
    "    math_problem = problem[\"question\"]\n",
    "    correct_answer = problem[\"answer\"].split(\"### \")[1]\n",
    "\n",
    "    human_message = HumanMessage(content=math_problem)\n",
    "\n",
    "    # Generate the model's response asynchronously\n",
    "    model_response = agent.invoke({\"task\": problem, \"messages\": [human_message]}, {\"recursion_limit\": 50})\n",
    "    # Extract the content of the last AI message\n",
    "    content = model_response[\"messages\"][-1].content\n",
    "\n",
    "    # Use regex to parse the numerical answer\n",
    "    try:\n",
    "        model_ans = re.search(r\"Answer:\\s*[^0-9]*([\\d]+(?:\\.\\d+)?)\", content).group(1).strip()\n",
    "        is_correct = check_answer(model_ans, correct_answer)\n",
    "    except Exception:\n",
    "        is_correct = False\n",
    "    return \"\\n\".join(m.pretty_repr() for m in model_response[\"messages\"]), is_correct, problem[\"question\"], problem[\"answer\"]\n",
    "\n",
    "\n",
    "def bench(val_sample):\n",
    "    k = 0\n",
    "    model_answers = []\n",
    "\n",
    "    # Process all tasks with progress tracking\n",
    "    for problem in tqdm(val_sample, total=len(val_sample)):\n",
    "        content, is_correct, problem, answer = process_problem(problem)\n",
    "        model_answers.append({\n",
    "            \"Agent answer\": content,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"Problem\": problem,\n",
    "            \"Right Answer\": answer\n",
    "        })\n",
    "        if is_correct:\n",
    "            k += 1\n",
    "    print(f\"Precision: {k / len(val_sample)}\")\n",
    "\n",
    "    return pd.DataFrame(model_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:52<00:00,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = bench(val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a symbol for proper import in Excel\n",
    "results['Agent answer'] = results['Agent answer'].apply(lambda x: '-' + x)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results.to_excel(\"results/rewoo_agent_bench.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
